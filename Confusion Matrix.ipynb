{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix and other ways of finding out how well your model performs\n",
    "\n",
    "Error.  \n",
    "How do we define it?  \n",
    "For a single prediction, we can say:  \n",
    "`error = actual value - predicted value`  \n",
    "or in cases of classification, we can say:  \n",
    "` error = count_of(actual value != predicted value) for each class ` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underfitting and Overfitting\n",
    "\n",
    "A good question to ask ourselves is \"What is learning?\"\n",
    "Learning is when the system can \"generalize\" the patterns that it has learned and make predictions for data that it has never seen before. \n",
    "\n",
    "However, depending upon how the its built, there are limits to how much a model can generalize.\n",
    "\n",
    "### Overfitting\n",
    "If the model matches the training data very closely, almost perfectly, there's a good chance that it'll perform poorly when we validate it against test data. This poor performance is because the model cannot generalize what it has enough to answer questions raised by the test data.\n",
    "\n",
    "Think of it as a tightly fit suit. It'll look very good at the tailor's but may not work for you once you bring it home.\n",
    "\n",
    "### Underfitting\n",
    "The model is *too* general - not grasping the patterns and idiosyncracies of the training data. Thus, it performs poorly in tests because it predicts values that are far off from reality. Many times these models perform poorly on training data too.  \n",
    "\n",
    "Think again of a suit that is several sizes bigger - sure you can wear it now or at home, even when you gain some weight, but it'll never serve it's purpose.  \n",
    "\n",
    "**The suit must be just right.**  \n",
    "This is the balancing act that we care about.\n",
    "\n",
    "These are the fundamental components of the Confusion matrix for *binary classification*:\n",
    "- condition positive (P) = the number of real positive cases in the data\n",
    "- condition negative (N) = the number of real negative cases in the data\n",
    "- predicted positive (P')\n",
    "- predicted negative (N')\n",
    "- true positive (TP) = The prediction was positive correctly matches the reality (we said it would happen and it did!)\n",
    "- true negative (TN) = The prediction was -ve and correctly matches reality (we told you it would not happen and it did not)\n",
    "- false positive (FP) = We said it would happen but it did not. Also called Type 1 error.\n",
    "- false negative (FN) = We said it would not happen but it did. Type 2 error.\n",
    "\n",
    "With the above in mind, there's a ton of other definitions that help us figure out how effective the model is:\n",
    "- Accuracy: Overall, how often is the model correct? (TP+TN)/total\n",
    "- Misclassification Rate or Error Rate: how often is it wrong? (FP+FN)/total\n",
    "- True Positive Rate or \"Sensitivity\" or \"Recall\": When it's actually yes, how often does it predict yes? TP/P\n",
    "- False Positive Rate: When it's actually no, how often does it predict yes? FP/N\n",
    "- True Negative Rate or \"Specificity\": When it's actually no, how often does it predict no? TN/N\n",
    "- Precision: When it predicts yes, how often is it correct? TP/P'\n",
    "- Prevalence: How often does the yes condition actually occur in our sample? P/total\n",
    "\n",
    "These definitions are slightly more involved for multi-class classification problems [WIP]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
